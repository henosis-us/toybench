# ToyBench: Evaluating Agentic Planning, Action, and Iterative Refinement

## Vision & Goals

**ToyBench** evaluates the capability of Large Language Models (LLMs) to **plan, take actions agentically over time, and refine outputs based on feedback**. Our vision is to create a standardized method for measuring how well LLMs translate complex requirements and high-level goals into tangible, **"valid" sequences of actions or artifacts** within defined environments, assessed through a practical, multi-step process that may involve interaction and iteration.

**Core Goals:**

1.  **Evaluate Agentic Capabilities:** Assess the LLM's ability to act as an agent â€“ perceiving environmental state (which may include feedback), planning sequences of steps or refinements, and executing actions or generating artifacts to achieve objectives.
2.  **Measure Planning & Reasoning:** Test the quality of the agent's multi-step planning, reasoning, and ability to incorporate feedback within the constraints and dynamics of the task environment.
3.  **Assess Action/Output Validity & Effectiveness:** Determine if the agent's chosen actions are permissible by the environment's rules (e.g., valid commands) or if generated artifacts (e.g., code) are syntactically valid and contribute effectively towards the stated goal over multiple turns or refinement cycles.
4.  **Benchmark Goal Achievement:** Quantify the agent's success rate in reaching the desired end state or fulfilling task requirements across multiple independent attempts, potentially after a fixed number of refinement opportunities.
5.  **Analyze Performance Horizons / Refinement Cycles:** Understand how agent performance changes based on the number of allowed interaction steps or refinement rounds (short vs. medium-term planning and iteration).
6.  **Compare Agent Performance:** Provide a consistent framework to compare different LLMs or prompting strategies on their agentic performance across various "toy problems".

## Core Concepts

*   **Agent:** The LLM being evaluated.
*   **Environment:** A simulated context (e.g., game board, text world, simulated file system, browser rendering loop) with defined rules, states, and potentially feedback mechanisms.
*   **State:** The current situation within the environment, provided as input to the agent at each step. This might include direct observations (like a game board) or structured feedback (like evaluation results and logs).
*   **Goal:** The high-level objective the agent must achieve.
*   **Actions:** Permissible operations the agent can choose to alter the environment's state (e.g., `place O at row,col`, `mv file dest`) or the primary output generated by the agent in a turn (e.g., a block of HTML/JS code). The agent outputs its chosen action or artifact.
*   **Turns/Steps/Rounds:** The number of interactions (State -> Action -> New State/Feedback) the agent takes. For iterative tasks like code generation, this often represents refinement cycles. The benchmark limits this via the `--rounds` parameter.
*   **Validity:** Whether an action chosen by the agent is allowed by the environment's rules (e.g., placing on an empty square) or whether a generated artifact meets basic structural requirements (e.g., non-empty code).
*   **Feedback (for iterative tasks):** Information provided back to the agent about the outcome of its previous action/generation, often combining automated checks (like browser logs) and evaluations (like LLM assessment of a screenshot).
*   **Rendering (for visual tasks):** Using external tools (like Selenium) to process the agent's output (e.g., HTML code) and produce a representation (e.g., a screenshot) for evaluation.
*   **Intermediate Evaluation (for iterative tasks):** An assessment performed within a multi-step process to provide feedback for the next iteration (e.g., evaluating a screenshot during code refinement).
*   **Final Evaluation:** An assessment performed at the end of an attempt to determine the final success score.

## Evaluation Methodology: Agent Interaction & Outcome Scoring

ToyBench evaluates the dynamic behavior of an LLM acting as an agent through interaction loops and outcome-based scoring. The specific process varies slightly depending on the task type (discrete action vs. iterative refinement).

**General Flow:**

1.  **Task Initialization:** The benchmark sets up a specific task environment (e.g., Tic-Tac-Toe board, initial file state, empty HTML canvas) and defines the agent's goal (e.g., "Win or Draw as Player O", "Organize files", "Generate a 3D solar system").
2.  **Agent Interaction Loop (per Attempt):**
    *   The agent receives the current environment state (e.g., board state, file listing, *or* feedback from the previous turn) and the goal.
    *   The agent outputs its chosen action (e.g., `place O at 1,1`) or artifact (e.g., HTML code).
    *   The action/artifact is validated against environment rules or basic structural checks. Invalid actions/outputs typically result in failure for that attempt or specific turn penalties (like re-prompting in TTT).
    *   If valid, the action updates the environment state directly (TTT, File System) OR triggers a process like rendering and intermediate evaluation (Solar System).
    *   The environment generates the new state or feedback for the next turn.
    *   This loop repeats up to a maximum number of turns/rounds (defined by `--rounds`) or until a terminal state (goal met, goal failed, critical invalid action, agent signals completion) is reached.
3.  **Data Capture:** The sequence of states, actions/artifacts, validity checks, feedback (if applicable), and evaluation results are logged for each attempt.
4.  **Final Outcome Scoring (per Attempt):** After each attempt concludes, the final state (e.g., final board, final file structure, *final screenshot*) is assessed against criteria defined in the `<task>_finaleval.txt` prompt or by deterministic checks within the environment. This results in a score for that single attempt:
    *   **Score 1 (Failure):** Goal not achieved, critical invalid action taken, failed due to rule violation, or final artifact/state fails evaluation criteria.
    *   **Score 2 (Partial / Valid but Incomplete):** Valid actions/outputs produced, some progress made, but goal not fully achieved within the turn/round limit (or achieved sub-optimally where applicable, or met partial criteria in the final evaluation).
    *   **Score 3 (Success):** The defined goal was successfully achieved through a sequence of valid actions/outputs within the turn/round limit, meeting the success criteria of the final evaluation.

## Reporting Metrics: Pass@K and Multi-Horizon/Round Analysis

To provide robust insights into agent performance, ToyBench uses the following reporting metrics, calculated from the outcome scores (where **Success = Score 3**):

*   **`pass@1` (Success Rate @ 1 Attempt):**
    *   Calculated as: `(Number of Successful Attempts) / (Total Attempts)` for a given task and turn/round limit (e.g., based on 20 attempts).
    *   Represents the probability that the agent succeeds on any single try. Measures **reliability**.
    *   Reported **averaged over all attempts** conducted for a specific setting.
*   **`pass@k` (Success Rate @ k Attempts):**
    *   Specifically, **`pass@20`** will typically be reported (using `k=20` attempts).
    *   Calculated as: `1` (or 100%) if *at least one* of the `k` attempts was successful, `0` (or 0%) otherwise.
    *   Measures whether the agent is **capable** of succeeding at least once given multiple tries.
*   **Multi-Turn Horizon / Refinement Round Analysis:**
    *   Recognizing that agent performance depends heavily on the planning horizon or number of refinement opportunities, metrics (`pass@1`, `pass@20`) will be reported **separately for different maximum turn/round limits**.
    *   Standard reporting horizons/rounds will be:
        *   **1 Turn/Round:** Tests immediate valid action selection or initial generation quality.
        *   **3 Turns/Rounds:** Tests very short-term planning or refinement capability.
        *   **5 Turns/Rounds:** Tests short-to-medium term planning, mistake avoidance, or iterative improvement.
    *   *This requires running the benchmark multiple times for the same task, varying the `--rounds` parameter (e.g., `--rounds 1`, `--rounds 3`, `--rounds 5`).*
*   **Overall ToyBench Score:**
    *   A final, aggregated score representing overall performance across the entire benchmark suite.
    *   Calculated as the **average `pass@1` score across all included tasks**, typically reported for a specific turn/round horizon (e.g., the 5-turn/round horizon results).
    *   Expressed as a percentage (e.g., average `pass@1` of 0.65 across 3 tasks = 65% ToyBench Score).

**Example Report Structure (Conceptual):**

| Task             | Max Turns/Rounds | `pass@1` (avg over 20) | `pass@20` (any success in 20) |
| :--------------- | :--------------- | :--------------------- | :---------------------------- |
| TicTacToe        | 1                | 95%                    | 100%                          |
| TicTacToe        | 3                | 80%                    | 100%                          |
| TicTacToe        | 5                | 70%                    | 100%                          |
| FileOrganizer    | 5                | 65%                    | 95%                           |
| FileOrganizer    | 10               | 60%                    | 95%                           |
| SolarGen         | 1                | 10%                    | 30%                           |
| SolarGen         | 3                | 35%                    | 65%                           |
| SolarGen         | 5                | 50%                    | 80%                           |
| **Overall (5 Rounds/Turns)** | **5**            | **X%**                 | **N/A**                       |

*(Note: Overall score typically focuses on average pass@1 for a consistent horizon/round count, e.g., 5).*

## Example Agentic Tasks

Tasks focus on planning, achieving goals through discrete actions, or iteratively generating artifacts based on feedback within defined environments.

---

### Task Example 1: Tic-Tac-Toe Player

*   **Goal:** Play Tic-Tac-Toe as Player 'X' against an optimal Minimax opponent ('O'), aiming to Win or Draw within the allowed turns.
*   **Environment:** 3x3 grid, game rules, deterministic optimal opponent.
*   **Actions:** `place X at row,col`.
*   **Final Scoring (`deterministic`):** Based on final board state (Win/Draw/Loss) and action validity log (Score 3 = Win or Draw with no invalid moves attempted).

---

### Task Example 2: Complex File Organizer (Simulated)

*   **Goal:** Parse instructions from a config file, conditionally move/copy/delete simulated files/directories, create new files with specific content, and end in a target directory, matching a complex final state.
*   **Environment:** Defined initial file/directory structure, allowed shell commands (`ls`, `cd`, `pwd`, `mkdir`, `cat`, `cp`, `rm`, `echo`), deterministic state updates.
*   **Actions:** Shell commands (e.g., `cp /project/src/main.py /archive/1.2/`, `echo "Done" > /final/status.txt`). Agent must signal `TASK_COMPLETE`.
*   **Final Scoring (`deterministic`):** Based on the final simulated file structure, file contents, and CWD matching the target state derived from the goal and initial config perfectly (Score 3 = Perfect match using valid commands and signaling completion).

---

### Task Example 3: Solar System Generator (Iterative Refinement)

*   **Goal:** Generate functional HTML and JavaScript (using Three.js) to render an interactive 3D solar system, iteratively improving the code based on visual feedback and browser logs over several rounds.
*   **Environment:** Browser rendering via Selenium, intermediate and final visual evaluation via a multimodal LLM.
*   **Action/Output:** Complete HTML/JS code block for the solar system simulation.
*   **Feedback:** A combination of (1) LLM evaluation of the rendered screenshot (using `solar_intermediate_eval.txt`) and (2) captured browser console logs.
*   **Final Scoring (`LLM Evaluation`):** Based on a multimodal LLM evaluating the *final* screenshot against criteria in `solar_finaleval.txt` after the specified number of refinement rounds (Score 3 = Meets 'excellent' criteria described in the prompt, e.g., >5 planets and moon visible).

---

## Prerequisites & Usage

**Prerequisites:**

*   **Python:** Version 3.9+ recommended.
*   **Dependencies:** Install required packages:
    ```bash
    pip install google-generativeai python-dotenv requests selenium # Add other LLM providers (e.g., openai) if needed
    ```
*   **API Keys:** Set environment variables for the LLM providers you intend to use (e.g., `GOOGLE_API_KEY`). Create a `.env` file in the project root for convenience:
    ```dotenv
    GOOGLE_API_KEY="your_google_api_key_here"
    # OPENAI_API_KEY="your_openai_api_key_here"
    ```
*   **WebDriver (for `solar_gen` task):**
    *   Install a web browser supported by Selenium (e.g., Google Chrome).
    *   Download the corresponding WebDriver executable (e.g., `chromedriver`) compatible with your browser version.
    *   Ensure the WebDriver executable is placed in your system's PATH or specify its location if needed (not currently supported via CLI arg).

**Usage:**

Run benchmarks specifying the task, provider, model, number of attempts, and the maximum turns/rounds:

```bash
# --- Discrete Action Tasks ---

# Test TicTacToe agent with a max horizon of 3 turns (run 20 times for metrics)
python toybench_cli.py --provider gemini --model gemini-1.5-flash-latest --task tic_tac_toe --attempts 20 --rounds 3

# Test FileOrganizer agent (runs until completion/error/limit) (run 20 times)
# --rounds is less critical here as FS has internal limits, but sets a fallback max
python toybench_cli.py --provider gemini --model gemini-1.5-pro-latest --task file_system --attempts 20 --rounds 50

# --- Iterative Refinement Task ---

# Test Solar System Generator with 5 refinement rounds (run 10 times for metrics)
# Ensure chromedriver is installed and in PATH
python toybench_cli.py --provider gemini --model gemini-2.5-pro-exp-03-25 --task solar_gen --attempts 10 --rounds 5

# --- General Options ---

# Run with debug logging
python toybench_cli.py --task tic_tac_toe --attempts 1 --rounds 3 --log_level DEBUG

# Specify a different output directory
python toybench_cli.py --task file_system --attempts 5 --output_dir my_custom_results